program: main.py
command:
  - ${env}
  - ${interpreter}
  - -O
  - ${program}
  - ${args}
metric:
  name: population/mean_f1
  goal: maximize
method: random
parameters:
  criterion_key:
    value: 'bce'
  random_seed:
    value: 100 #[ 4168, 3773, 4181, 2945, 4132, 1405, 4328, 1720, 1691, 78 ]
  general_model_config:
    parameters:
      patch_size:
        value: 8
      dim:
        value: 256
      depth:
        value: 2
      heads:
        value: 5
      mlp_dim:
        value: 256
      dropout:
        value: 0.21
      emb_dropout:
        value: 0.1735
  pretrain:
    parameters:
      epochs:
        value: 50
      batch_size:
        value: 128
      lr:
        value: 0.00083
      train_fraction:
        value: 0.7
      do_pretraining:
        value: True
  finetune:
    parameters:
      n_frozen_layers:
        value: 1
      num_episodes:
        value: 10
      epochs:
        value: 70
      lr:
        value: 0.0002269
      batch_size:
        value: 64
      do_finetuning:
        value: False
      do_swa:
        values: [ True, False ]
      swa_lrs:
        # value: 1e-2
        distribution: log_uniform_values
        min: 0.001
        max: 0.1
      swa_epoch_start:
        min: 0.7
        max: 0.95
      annealing_epochs: 
        values: [ 5, 10, 20 ]
  online:
    parameters:
      num_episodes:
        value: 0
        # min:
        #   1
        # max:
        #   30
      batch_size:
        value: 5
        # values: [ 5, 10 ]
      epochs:
        value: 20
      lr:
        value: 0.00045
        # distribution: log_uniform_values
        # min: 0.00008
        # max: 0.0007
      num_sessions:
        value: 3
      n_frozen_layers:
        value: 1
      train_intervals:
        value: 1
      first_training_episode:
        value: 0
      additional_train_episodes:
        value: 30
      adaptive_training:
        value: True
      balance_classes:
        value: False
      buffer_size:
        value: 3_000
      shuffle_episodes:
        value: True
      do_swa:
        values: [ True, False ]
      swa_lrs:
        # value: 1e-2
        distribution: log_uniform_values
        min: 0.001
        max: 0.1
      swa_epoch_start:
        min: 0.7
        max: 0.95
      annealing_epochs: 
        values: [ 5, 10, 20 ]
