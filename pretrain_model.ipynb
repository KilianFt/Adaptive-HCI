{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyoArmband Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'datasets/MyoArmbandDataset/PreTrainingDataset/'\n",
    "train_candidates = 12\n",
    "eval_path = 'datasets/MyoArmbandDataset/EvaluationDataset/'\n",
    "eval_candidates = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k8/_qw1v4gj2cv9cqh75_yrt3pc0000gn/T/ipykernel_31040/4057999897.py:33: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (example == []):\n"
     ]
    }
   ],
   "source": [
    "person_folders = os.listdir(eval_path)\n",
    "\n",
    "keys = ['training0', 'Test0', 'Test1']\n",
    "\n",
    "number_of_classes = 7\n",
    "number_of_vector_per_example = 52\n",
    "size_non_overlap = 5\n",
    "\n",
    "raw_dataset_dict = {}\n",
    "for key in keys:\n",
    "        \n",
    "    raw_dataset = {\n",
    "        'examples': [],\n",
    "        'labels': [],\n",
    "    }\n",
    "    list_dataset = []\n",
    "    list_labels = []\n",
    "    for person_dir in person_folders:\n",
    "        examples = []\n",
    "        labels = []\n",
    "        data_path = eval_path + person_dir + '/' + key\n",
    "        for data_file in os.listdir(data_path):\n",
    "            if (data_file.endswith(\".dat\")):\n",
    "                data_read_from_file = np.fromfile((data_path+'/'+data_file), dtype=np.int16)\n",
    "                data_read_from_file = np.array(data_read_from_file, dtype=np.float32)\n",
    "\n",
    "                dataset_example_formatted = []\n",
    "                example = []\n",
    "                emg_vector = []\n",
    "                for value in data_read_from_file:\n",
    "                    emg_vector.append(value)\n",
    "                    if (len(emg_vector) >= 8):\n",
    "                        if (example == []):\n",
    "                            example = emg_vector\n",
    "                        else:\n",
    "                            example = np.row_stack((example, emg_vector))\n",
    "                        emg_vector = []\n",
    "                        if (len(example) >= number_of_vector_per_example):\n",
    "                            example = example.transpose()\n",
    "                            dataset_example_formatted.append(example)\n",
    "                            example = example.transpose()\n",
    "                            example = example[size_non_overlap:]\n",
    "                dataset_example_formatted = np.array(dataset_example_formatted)\n",
    "                examples.append(dataset_example_formatted)\n",
    "                data_file_index = int(data_file.split('classe_')[1][:-4])\n",
    "                label = data_file_index % number_of_classes + np.zeros(dataset_example_formatted.shape[0])\n",
    "                labels.append(label)\n",
    "\n",
    "        raw_dataset['examples'].append(np.concatenate(examples))\n",
    "        raw_dataset['labels'].append(np.concatenate(labels))\n",
    "\n",
    "    raw_dataset_dict[key] = raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5306, 8, 52)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset_dict['training0']['examples'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomEMGDataset(Dataset):\n",
    "    def __init__(self, x_samples, y_samples):\n",
    "        self.x_samples = x_samples\n",
    "        self.y_samples = y_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_tensor = torch.tensor(self.x_samples[idx,:,:])\n",
    "        y_tensor = torch.tensor(self.y_samples[idx]).type(torch.LongTensor)\n",
    "        return x_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single person\n",
    "train_x = raw_dataset_dict['training0']['examples'][0]\n",
    "train_y = raw_dataset_dict['training0']['labels'][0]\n",
    "test_x = raw_dataset_dict['Test0']['examples'][0]\n",
    "test_y = raw_dataset_dict['Test0']['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all people\n",
    "train_x = np.concatenate(raw_dataset_dict['training0']['examples'])\n",
    "train_y = np.concatenate(raw_dataset_dict['training0']['labels'])\n",
    "test_x = np.concatenate(raw_dataset_dict['Test0']['examples'])\n",
    "test_y = np.concatenate(raw_dataset_dict['Test0']['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomEMGDataset(train_x, train_y)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = CustomEMGDataset(train_x, train_y)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(x[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        out = self.linear_relu_stack(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFNN(52*8, 128, number_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 7])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(train_features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/opt/homebrew/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  Referenced from: <06D2C3BD-26E5-3AB9-A866-63839BE393A7> /opt/homebrew/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <59ED1CF5-3CD8-3592-A70B-3AB98E4C5F21> /opt/homebrew/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "model = ViT(\n",
    "    image_size = 52,\n",
    "    patch_size = 4,\n",
    "    num_classes = 7,\n",
    "    dim = 64,\n",
    "    depth = 1,\n",
    "    heads = 2,\n",
    "    mlp_dim = 128,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    channels=1,\n",
    ")\n",
    "\n",
    "img = torch.randn(10, 1, 8, 52)\n",
    "\n",
    "preds = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ViT'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 1.92876304\n",
      "[1,   200] loss: 1.73230485\n",
      "[1,   300] loss: 1.60721234\n",
      "[1,   400] loss: 1.56392165\n",
      "[1,   500] loss: 1.48821062\n",
      "[1,   600] loss: 1.42310641\n",
      "[1,   700] loss: 1.41488457\n",
      "[1,   800] loss: 1.36426049\n",
      "[1,   900] loss: 1.36531197\n",
      "[1,  1000] loss: 1.35037308\n",
      "[1,  1100] loss: 1.34844885\n",
      "[1,  1200] loss: 1.31014080\n",
      "[1,  1300] loss: 1.31217722\n",
      "[1,  1400] loss: 1.29279832\n",
      "[1,  1500] loss: 1.29502750\n",
      "[1,  1600] loss: 1.30136244\n",
      "[1,  1700] loss: 1.29493697\n",
      "[1,  1800] loss: 1.27477097\n",
      "[1,  1900] loss: 1.25018238\n",
      "[1,  2000] loss: 1.25360665\n",
      "[1,  2100] loss: 1.22789168\n",
      "[1,  2200] loss: 1.27182587\n",
      "[1,  2300] loss: 1.22589642\n",
      "[1,  2400] loss: 1.24131900\n",
      "[1,  2500] loss: 1.22445934\n",
      "[1,  2600] loss: 1.19964811\n",
      "[1,  2700] loss: 1.21086208\n",
      "[1,  2800] loss: 1.21030939\n",
      "[1,  2900] loss: 1.21675210\n",
      "test accuracy 0.5995556063454457\n",
      "[2,   100] loss: 1.19717006\n",
      "[2,   200] loss: 1.21257733\n",
      "[2,   300] loss: 1.18270198\n",
      "[2,   400] loss: 1.16944089\n",
      "[2,   500] loss: 1.17104893\n",
      "[2,   600] loss: 1.17410038\n",
      "[2,   700] loss: 1.20417745\n",
      "[2,   800] loss: 1.14343279\n",
      "[2,   900] loss: 1.18140283\n",
      "[2,  1000] loss: 1.13589918\n",
      "[2,  1100] loss: 1.16338474\n",
      "[2,  1200] loss: 1.17509487\n",
      "[2,  1300] loss: 1.13394728\n",
      "[2,  1400] loss: 1.14807845\n",
      "[2,  1500] loss: 1.15741601\n",
      "[2,  1600] loss: 1.11094684\n",
      "[2,  1700] loss: 1.13725652\n",
      "[2,  1800] loss: 1.15184814\n",
      "[2,  1900] loss: 1.14805601\n",
      "[2,  2000] loss: 1.13959170\n",
      "[2,  2100] loss: 1.15383133\n",
      "[2,  2200] loss: 1.11519948\n",
      "[2,  2300] loss: 1.12932962\n",
      "[2,  2400] loss: 1.13555724\n",
      "[2,  2500] loss: 1.13474563\n",
      "[2,  2600] loss: 1.09209797\n",
      "[2,  2700] loss: 1.10289489\n",
      "[2,  2800] loss: 1.12053102\n",
      "[2,  2900] loss: 1.09375464\n",
      "test accuracy 0.6470844814709897\n",
      "[3,   100] loss: 1.10858550\n",
      "[3,   200] loss: 1.06209491\n",
      "[3,   300] loss: 1.08000850\n",
      "[3,   400] loss: 1.09692124\n",
      "[3,   500] loss: 1.08050340\n",
      "[3,   600] loss: 1.09755063\n",
      "[3,   700] loss: 1.09220893\n",
      "[3,   800] loss: 1.09682796\n",
      "[3,   900] loss: 1.05110931\n",
      "[3,  1000] loss: 1.09298636\n",
      "[3,  1100] loss: 1.07908631\n",
      "[3,  1200] loss: 1.10245803\n",
      "[3,  1300] loss: 1.07975702\n",
      "[3,  1400] loss: 1.06214185\n",
      "[3,  1500] loss: 1.04126226\n",
      "[3,  1600] loss: 1.05152869\n",
      "[3,  1700] loss: 1.05600591\n",
      "[3,  1800] loss: 1.05816439\n",
      "[3,  1900] loss: 1.07222078\n",
      "[3,  2000] loss: 1.05148033\n",
      "[3,  2100] loss: 1.05919138\n",
      "[3,  2200] loss: 1.04369255\n",
      "[3,  2300] loss: 1.02858738\n",
      "[3,  2400] loss: 1.07326727\n",
      "[3,  2500] loss: 1.01477072\n",
      "[3,  2600] loss: 1.06367006\n",
      "[3,  2700] loss: 1.02795693\n",
      "[3,  2800] loss: 1.05652558\n",
      "[3,  2900] loss: 1.05964731\n",
      "test accuracy 0.6657768599830033\n",
      "[4,   100] loss: 1.00179178\n",
      "[4,   200] loss: 1.05263631\n",
      "[4,   300] loss: 1.04535902\n",
      "[4,   400] loss: 1.04693273\n",
      "[4,   500] loss: 1.02968463\n",
      "[4,   600] loss: 1.00956586\n",
      "[4,   700] loss: 1.02623488\n",
      "[4,   800] loss: 1.01853835\n",
      "[4,   900] loss: 1.00508285\n",
      "[4,  1000] loss: 0.99403171\n",
      "[4,  1100] loss: 1.02627305\n",
      "[4,  1200] loss: 1.00205221\n",
      "[4,  1300] loss: 1.03430165\n",
      "[4,  1400] loss: 1.04549029\n",
      "[4,  1500] loss: 1.00059135\n",
      "[4,  1600] loss: 1.03096929\n",
      "[4,  1700] loss: 1.01643855\n",
      "[4,  1800] loss: 1.00381530\n",
      "[4,  1900] loss: 1.05342847\n",
      "[4,  2000] loss: 1.00524457\n",
      "[4,  2100] loss: 1.00089300\n",
      "[4,  2200] loss: 1.01886402\n",
      "[4,  2300] loss: 1.01082088\n",
      "[4,  2400] loss: 0.97845935\n",
      "[4,  2500] loss: 0.95878562\n",
      "[4,  2600] loss: 0.99442935\n",
      "[4,  2700] loss: 1.00279819\n",
      "[4,  2800] loss: 1.00036024\n",
      "[4,  2900] loss: 0.96085779\n",
      "test accuracy 0.678993587597538\n",
      "[5,   100] loss: 0.95896580\n",
      "[5,   200] loss: 0.98329072\n",
      "[5,   300] loss: 0.98266693\n",
      "[5,   400] loss: 0.98112420\n",
      "[5,   500] loss: 1.00946917\n",
      "[5,   600] loss: 0.97991630\n",
      "[5,   700] loss: 0.96805534\n",
      "[5,   800] loss: 0.95412443\n",
      "[5,   900] loss: 0.95671610\n",
      "[5,  1000] loss: 0.98023115\n",
      "[5,  1100] loss: 0.96740479\n",
      "[5,  1200] loss: 0.97282967\n",
      "[5,  1300] loss: 0.94164285\n",
      "[5,  1400] loss: 0.94077472\n",
      "[5,  1500] loss: 0.97781870\n",
      "[5,  1600] loss: 0.96064034\n",
      "[5,  1700] loss: 0.96040729\n",
      "[5,  1800] loss: 0.95751759\n",
      "[5,  1900] loss: 1.00941127\n",
      "[5,  2000] loss: 0.99672455\n",
      "[5,  2100] loss: 0.96588268\n",
      "[5,  2200] loss: 0.97909134\n",
      "[5,  2300] loss: 0.99342726\n",
      "[5,  2400] loss: 0.93368541\n",
      "[5,  2500] loss: 0.95319417\n",
      "[5,  2600] loss: 0.99132462\n",
      "[5,  2700] loss: 0.95176878\n",
      "[5,  2800] loss: 0.94061570\n",
      "[5,  2900] loss: 0.96941030\n",
      "test accuracy 0.7008881434936005\n",
      "[6,   100] loss: 0.96347817\n",
      "[6,   200] loss: 0.95616771\n",
      "[6,   300] loss: 0.90033166\n",
      "[6,   400] loss: 0.93061634\n",
      "[6,   500] loss: 0.93444848\n",
      "[6,   600] loss: 0.94636329\n",
      "[6,   700] loss: 0.93647392\n",
      "[6,   800] loss: 0.95607406\n",
      "[6,   900] loss: 0.93916163\n",
      "[6,  1000] loss: 0.95132053\n",
      "[6,  1100] loss: 0.94230726\n",
      "[6,  1200] loss: 0.90989886\n",
      "[6,  1300] loss: 0.95126913\n",
      "[6,  1400] loss: 0.91112773\n",
      "[6,  1500] loss: 0.93810931\n",
      "[6,  1600] loss: 0.97732293\n",
      "[6,  1700] loss: 0.95115138\n",
      "[6,  1800] loss: 0.91585606\n",
      "[6,  1900] loss: 0.94966960\n",
      "[6,  2000] loss: 0.93205266\n",
      "[6,  2100] loss: 0.92301342\n",
      "[6,  2200] loss: 0.92943059\n",
      "[6,  2300] loss: 0.88156773\n",
      "[6,  2400] loss: 0.93240143\n",
      "[6,  2500] loss: 0.95308973\n",
      "[6,  2600] loss: 0.92185100\n",
      "[6,  2700] loss: 0.93158836\n",
      "[6,  2800] loss: 0.93562849\n",
      "[6,  2900] loss: 0.90783856\n",
      "test accuracy 0.692436455409338\n",
      "[7,   100] loss: 0.88603062\n",
      "[7,   200] loss: 0.92394275\n",
      "[7,   300] loss: 0.92642174\n",
      "[7,   400] loss: 0.90555670\n",
      "[7,   500] loss: 0.90064384\n",
      "[7,   600] loss: 0.88109808\n",
      "[7,   700] loss: 0.93717275\n",
      "[7,   800] loss: 0.94315111\n",
      "[7,   900] loss: 0.90551682\n",
      "[7,  1000] loss: 0.91803176\n",
      "[7,  1100] loss: 0.90089946\n",
      "[7,  1200] loss: 0.90813908\n",
      "[7,  1300] loss: 0.92421920\n",
      "[7,  1400] loss: 0.92488009\n",
      "[7,  1500] loss: 0.91957099\n",
      "[7,  1600] loss: 0.93181296\n",
      "[7,  1700] loss: 0.90557506\n",
      "[7,  1800] loss: 0.90674442\n",
      "[7,  1900] loss: 0.92264053\n",
      "[7,  2000] loss: 0.89413866\n",
      "[7,  2100] loss: 0.90591979\n",
      "[7,  2200] loss: 0.91470991\n",
      "[7,  2300] loss: 0.84805296\n",
      "[7,  2400] loss: 0.90025331\n",
      "[7,  2500] loss: 0.88804534\n",
      "[7,  2600] loss: 0.92675877\n",
      "[7,  2700] loss: 0.91912354\n",
      "[7,  2800] loss: 0.89213256\n",
      "[7,  2900] loss: 0.89633229\n",
      "test accuracy 0.71084233602019\n",
      "[8,   100] loss: 0.88693882\n",
      "[8,   200] loss: 0.89196033\n",
      "[8,   300] loss: 0.90447303\n",
      "[8,   400] loss: 0.86649919\n",
      "[8,   500] loss: 0.89621113\n",
      "[8,   600] loss: 0.89268358\n",
      "[8,   700] loss: 0.89700607\n",
      "[8,   800] loss: 0.89535259\n",
      "[8,   900] loss: 0.87892972\n",
      "[8,  1000] loss: 0.90125285\n",
      "[8,  1100] loss: 0.90410431\n",
      "[8,  1200] loss: 0.87550566\n",
      "[8,  1300] loss: 0.89305332\n",
      "[8,  1400] loss: 0.91968428\n",
      "[8,  1500] loss: 0.86907182\n",
      "[8,  1600] loss: 0.92583347\n",
      "[8,  1700] loss: 0.87330236\n",
      "[8,  1800] loss: 0.86007436\n",
      "[8,  1900] loss: 0.88747021\n",
      "[8,  2000] loss: 0.89234777\n",
      "[8,  2100] loss: 0.88231028\n",
      "[8,  2200] loss: 0.88014831\n",
      "[8,  2300] loss: 0.89361313\n",
      "[8,  2400] loss: 0.89368092\n",
      "[8,  2500] loss: 0.86782516\n",
      "[8,  2600] loss: 0.90420311\n",
      "[8,  2700] loss: 0.90194953\n",
      "[8,  2800] loss: 0.87654004\n",
      "[8,  2900] loss: 0.89196893\n",
      "test accuracy 0.7320117947001107\n",
      "[9,   100] loss: 0.86673258\n",
      "[9,   200] loss: 0.87742691\n",
      "[9,   300] loss: 0.86405547\n",
      "[9,   400] loss: 0.86659838\n",
      "[9,   500] loss: 0.86396005\n",
      "[9,   600] loss: 0.88614198\n",
      "[9,   700] loss: 0.88290548\n",
      "[9,   800] loss: 0.88224576\n",
      "[9,   900] loss: 0.86994524\n",
      "[9,  1000] loss: 0.88850697\n",
      "[9,  1100] loss: 0.85009118\n",
      "[9,  1200] loss: 0.87519316\n",
      "[9,  1300] loss: 0.88042273\n",
      "[9,  1400] loss: 0.86679983\n",
      "[9,  1500] loss: 0.85893697\n",
      "[9,  1600] loss: 0.88090174\n",
      "[9,  1700] loss: 0.85410504\n",
      "[9,  1800] loss: 0.89107895\n",
      "[9,  1900] loss: 0.84991271\n",
      "[9,  2000] loss: 0.86960522\n",
      "[9,  2100] loss: 0.88554599\n",
      "[9,  2200] loss: 0.86051089\n",
      "[9,  2300] loss: 0.87160954\n",
      "[9,  2400] loss: 0.86025301\n",
      "[9,  2500] loss: 0.86134622\n",
      "[9,  2600] loss: 0.87834871\n",
      "[9,  2700] loss: 0.85503013\n",
      "[9,  2800] loss: 0.85254927\n",
      "[9,  2900] loss: 0.82023171\n",
      "test accuracy 0.7337146867193738\n",
      "[10,   100] loss: 0.81257617\n",
      "[10,   200] loss: 0.88893909\n",
      "[10,   300] loss: 0.84636829\n",
      "[10,   400] loss: 0.88890880\n",
      "[10,   500] loss: 0.84934194\n",
      "[10,   600] loss: 0.89200415\n",
      "[10,   700] loss: 0.84255878\n",
      "[10,   800] loss: 0.85072823\n",
      "[10,   900] loss: 0.84087358\n",
      "[10,  1000] loss: 0.81707199\n",
      "[10,  1100] loss: 0.84256190\n",
      "[10,  1200] loss: 0.84801016\n",
      "[10,  1300] loss: 0.83694809\n",
      "[10,  1400] loss: 0.83392954\n",
      "[10,  1500] loss: 0.84166407\n",
      "[10,  1600] loss: 0.84340947\n",
      "[10,  1700] loss: 0.84708590\n",
      "[10,  1800] loss: 0.83257342\n",
      "[10,  1900] loss: 0.83833498\n",
      "[10,  2000] loss: 0.81767459\n",
      "[10,  2100] loss: 0.84764043\n",
      "[10,  2200] loss: 0.82715627\n",
      "[10,  2300] loss: 0.86618094\n",
      "[10,  2400] loss: 0.80609940\n",
      "[10,  2500] loss: 0.81552056\n",
      "[10,  2600] loss: 0.83621705\n",
      "[10,  2700] loss: 0.82988806\n",
      "[10,  2800] loss: 0.80471441\n",
      "[10,  2900] loss: 0.84097233\n",
      "test accuracy 0.7246336689758184\n",
      "[11,   100] loss: 0.80096686\n",
      "[11,   200] loss: 0.84317059\n",
      "[11,   300] loss: 0.81383428\n",
      "[11,   400] loss: 0.82026274\n",
      "[11,   500] loss: 0.81216942\n",
      "[11,   600] loss: 0.79592909\n",
      "[11,   700] loss: 0.79718944\n",
      "[11,   800] loss: 0.82091822\n",
      "[11,   900] loss: 0.84047415\n",
      "[11,  1000] loss: 0.81519597\n",
      "[11,  1100] loss: 0.82630181\n",
      "[11,  1200] loss: 0.79225494\n",
      "[11,  1300] loss: 0.82502934\n",
      "[11,  1400] loss: 0.80821455\n",
      "[11,  1500] loss: 0.82587592\n",
      "[11,  1600] loss: 0.81453507\n",
      "[11,  1700] loss: 0.80818767\n",
      "[11,  1800] loss: 0.83934818\n",
      "[11,  1900] loss: 0.80073137\n",
      "[11,  2000] loss: 0.78376097\n",
      "[11,  2100] loss: 0.82221532\n",
      "[11,  2200] loss: 0.81905789\n",
      "[11,  2300] loss: 0.79385272\n",
      "[11,  2400] loss: 0.77183357\n",
      "[11,  2500] loss: 0.78902836\n",
      "[11,  2600] loss: 0.76977754\n",
      "[11,  2700] loss: 0.78723129\n",
      "[11,  2800] loss: 0.77936747\n",
      "[11,  2900] loss: 0.78050826\n",
      "test accuracy 0.7335601709973989\n",
      "[12,   100] loss: 0.79287827\n",
      "[12,   200] loss: 0.76705235\n",
      "[12,   300] loss: 0.77275289\n",
      "[12,   400] loss: 0.77823285\n",
      "[12,   500] loss: 0.79369450\n",
      "[12,   600] loss: 0.76213729\n",
      "[12,   700] loss: 0.79267984\n",
      "[12,   800] loss: 0.77175375\n",
      "[12,   900] loss: 0.77715860\n",
      "[12,  1000] loss: 0.76670313\n",
      "[12,  1100] loss: 0.72470605\n",
      "[12,  1200] loss: 0.74612564\n",
      "[12,  1300] loss: 0.76209277\n",
      "[12,  1400] loss: 0.78061803\n",
      "[12,  1500] loss: 0.77354211\n",
      "[12,  1600] loss: 0.78164511\n",
      "[12,  1700] loss: 0.75084635\n",
      "[12,  1800] loss: 0.78914733\n",
      "[12,  1900] loss: 0.73966278\n",
      "[12,  2000] loss: 0.75782874\n",
      "[12,  2100] loss: 0.80081872\n",
      "[12,  2200] loss: 0.74171546\n",
      "[12,  2300] loss: 0.76911483\n",
      "[12,  2400] loss: 0.77951752\n",
      "[12,  2500] loss: 0.74798394\n",
      "[12,  2600] loss: 0.72896816\n",
      "[12,  2700] loss: 0.74650069\n",
      "[12,  2800] loss: 0.72932576\n",
      "[12,  2900] loss: 0.73649745\n",
      "test accuracy 0.7639555059102264\n",
      "[13,   100] loss: 0.72947803\n",
      "[13,   200] loss: 0.75059191\n",
      "[13,   300] loss: 0.72510335\n",
      "[13,   400] loss: 0.74854252\n",
      "[13,   500] loss: 0.72661261\n",
      "[13,   600] loss: 0.73439092\n",
      "[13,   700] loss: 0.72197444\n",
      "[13,   800] loss: 0.74815594\n",
      "[13,   900] loss: 0.70295823\n",
      "[13,  1000] loss: 0.71840579\n",
      "[13,  1100] loss: 0.69732929\n",
      "[13,  1200] loss: 0.72089808\n",
      "[13,  1300] loss: 0.70930015\n",
      "[13,  1400] loss: 0.72088005\n",
      "[13,  1500] loss: 0.70146411\n",
      "[13,  1600] loss: 0.72195097\n",
      "[13,  1700] loss: 0.69569430\n",
      "[13,  1800] loss: 0.75331640\n",
      "[13,  1900] loss: 0.69884317\n",
      "[13,  2000] loss: 0.70348119\n",
      "[13,  2100] loss: 0.72597879\n",
      "[13,  2200] loss: 0.69132470\n",
      "[13,  2300] loss: 0.69076608\n",
      "[13,  2400] loss: 0.71076335\n",
      "[13,  2500] loss: 0.68156300\n",
      "[13,  2600] loss: 0.71811359\n",
      "[13,  2700] loss: 0.71739800\n",
      "[13,  2800] loss: 0.70063507\n",
      "[13,  2900] loss: 0.67647955\n",
      "test accuracy 0.7206870799103808\n",
      "[14,   100] loss: 0.66021654\n",
      "[14,   200] loss: 0.70357589\n",
      "[14,   300] loss: 0.69640028\n",
      "[14,   400] loss: 0.70322058\n",
      "[14,   500] loss: 0.69677232\n",
      "[14,   600] loss: 0.67801804\n",
      "[14,   700] loss: 0.69046583\n",
      "[14,   800] loss: 0.66875048\n",
      "[14,   900] loss: 0.68406073\n",
      "[14,  1000] loss: 0.71898063\n",
      "[14,  1100] loss: 0.68436448\n",
      "[14,  1200] loss: 0.65114494\n",
      "[14,  1300] loss: 0.68552876\n",
      "[14,  1400] loss: 0.69024036\n",
      "[14,  1500] loss: 0.66688257\n",
      "[14,  1600] loss: 0.68451641\n",
      "[14,  1700] loss: 0.70138696\n",
      "[14,  1800] loss: 0.69575755\n",
      "[14,  1900] loss: 0.71974427\n",
      "[14,  2000] loss: 0.67718716\n",
      "[14,  2100] loss: 0.65639920\n",
      "[14,  2200] loss: 0.70959624\n",
      "[14,  2300] loss: 0.68264592\n",
      "[14,  2400] loss: 0.66211532\n",
      "[14,  2500] loss: 0.66745502\n",
      "[14,  2600] loss: 0.65857139\n",
      "[14,  2700] loss: 0.65251613\n",
      "[14,  2800] loss: 0.64334488\n",
      "[14,  2900] loss: 0.68548800\n",
      "test accuracy 0.7599767260693775\n",
      "[15,   100] loss: 0.66551248\n",
      "[15,   200] loss: 0.67069361\n",
      "[15,   300] loss: 0.69828955\n",
      "[15,   400] loss: 0.68344678\n",
      "[15,   500] loss: 0.68273331\n",
      "[15,   600] loss: 0.65410055\n",
      "[15,   700] loss: 0.65235722\n",
      "[15,   800] loss: 0.65298450\n",
      "[15,   900] loss: 0.66524184\n",
      "[15,  1000] loss: 0.63317769\n",
      "[15,  1100] loss: 0.66310360\n",
      "[15,  1200] loss: 0.64031385\n",
      "[15,  1300] loss: 0.70788318\n",
      "[15,  1400] loss: 0.67293167\n",
      "[15,  1500] loss: 0.65505334\n",
      "[15,  1600] loss: 0.65057239\n",
      "[15,  1700] loss: 0.64553003\n",
      "[15,  1800] loss: 0.61764846\n",
      "[15,  1900] loss: 0.66808041\n",
      "[15,  2000] loss: 0.65434736\n",
      "[15,  2100] loss: 0.70073701\n",
      "[15,  2200] loss: 0.65765748\n",
      "[15,  2300] loss: 0.62715273\n",
      "[15,  2400] loss: 0.64399920\n",
      "[15,  2500] loss: 0.64931390\n",
      "[15,  2600] loss: 0.66198911\n",
      "[15,  2700] loss: 0.64458461\n",
      "[15,  2800] loss: 0.64818550\n",
      "[15,  2900] loss: 0.64069392\n",
      "test accuracy 0.7906014202570112\n",
      "[16,   100] loss: 0.65683477\n",
      "[16,   200] loss: 0.66588778\n",
      "[16,   300] loss: 0.67791105\n",
      "[16,   400] loss: 0.65163287\n",
      "[16,   500] loss: 0.66016820\n",
      "[16,   600] loss: 0.63833983\n",
      "[16,   700] loss: 0.64034303\n",
      "[16,   800] loss: 0.63772815\n",
      "[16,   900] loss: 0.62427747\n",
      "[16,  1000] loss: 0.69118654\n",
      "[16,  1100] loss: 0.65310543\n",
      "[16,  1200] loss: 0.60859747\n",
      "[16,  1300] loss: 0.62951460\n",
      "[16,  1400] loss: 0.63315072\n",
      "[16,  1500] loss: 0.65940618\n",
      "[16,  1600] loss: 0.62479370\n",
      "[16,  1700] loss: 0.63353925\n",
      "[16,  1800] loss: 0.65464613\n",
      "[16,  1900] loss: 0.64841913\n",
      "[16,  2000] loss: 0.62979045\n",
      "[16,  2100] loss: 0.61737959\n",
      "[16,  2200] loss: 0.63799136\n",
      "[16,  2300] loss: 0.64764250\n",
      "[16,  2400] loss: 0.63098975\n",
      "[16,  2500] loss: 0.63157538\n",
      "[16,  2600] loss: 0.65700714\n",
      "[16,  2700] loss: 0.63365897\n",
      "[16,  2800] loss: 0.62722634\n",
      "[16,  2900] loss: 0.61958300\n",
      "test accuracy 0.7511129960598492\n",
      "[17,   100] loss: 0.61160035\n",
      "[17,   200] loss: 0.65579269\n",
      "[17,   300] loss: 0.62304600\n",
      "[17,   400] loss: 0.62958985\n",
      "[17,   500] loss: 0.60849080\n",
      "[17,   600] loss: 0.64531401\n",
      "[17,   700] loss: 0.64397179\n",
      "[17,   800] loss: 0.66743081\n",
      "[17,   900] loss: 0.64666361\n",
      "[17,  1000] loss: 0.61745172\n",
      "[17,  1100] loss: 0.61334074\n",
      "[17,  1200] loss: 0.60212967\n",
      "[17,  1300] loss: 0.62294359\n",
      "[17,  1400] loss: 0.62589621\n",
      "[17,  1500] loss: 0.58969362\n",
      "[17,  1600] loss: 0.61261598\n",
      "[17,  1700] loss: 0.63368520\n",
      "[17,  1800] loss: 0.63138142\n",
      "[17,  1900] loss: 0.66209518\n",
      "[17,  2000] loss: 0.60961926\n",
      "[17,  2100] loss: 0.63211087\n",
      "[17,  2200] loss: 0.62993448\n",
      "[17,  2300] loss: 0.63886919\n",
      "[17,  2400] loss: 0.60278897\n",
      "[17,  2500] loss: 0.60461902\n",
      "[17,  2600] loss: 0.60973056\n",
      "[17,  2700] loss: 0.62772528\n",
      "[17,  2800] loss: 0.59554489\n",
      "[17,  2900] loss: 0.62346286\n",
      "test accuracy 0.784447348767737\n",
      "[18,   100] loss: 0.59343743\n",
      "[18,   200] loss: 0.64035122\n",
      "[18,   300] loss: 0.57918151\n",
      "[18,   400] loss: 0.60686363\n",
      "[18,   500] loss: 0.61953194\n",
      "[18,   600] loss: 0.60232120\n",
      "[18,   700] loss: 0.63902153\n",
      "[18,   800] loss: 0.61746850\n",
      "[18,   900] loss: 0.62069437\n",
      "[18,  1000] loss: 0.62767330\n",
      "[18,  1100] loss: 0.61939161\n",
      "[18,  1200] loss: 0.58196154\n",
      "[18,  1300] loss: 0.64569063\n",
      "[18,  1400] loss: 0.62252454\n",
      "[18,  1500] loss: 0.62766119\n",
      "[18,  1600] loss: 0.61753095\n",
      "[18,  1700] loss: 0.62958440\n",
      "[18,  1800] loss: 0.58484985\n",
      "[18,  1900] loss: 0.61865303\n",
      "[18,  2000] loss: 0.59994190\n",
      "[18,  2100] loss: 0.59804767\n",
      "[18,  2200] loss: 0.63301859\n",
      "[18,  2300] loss: 0.60915209\n",
      "[18,  2400] loss: 0.59619538\n",
      "[18,  2500] loss: 0.61470585\n",
      "[18,  2600] loss: 0.60133444\n",
      "[18,  2700] loss: 0.64460693\n",
      "[18,  2800] loss: 0.60214645\n",
      "[18,  2900] loss: 0.63189265\n",
      "test accuracy 0.7784791790064639\n",
      "[19,   100] loss: 0.59642671\n",
      "[19,   200] loss: 0.61009027\n",
      "[19,   300] loss: 0.65007172\n",
      "[19,   400] loss: 0.61427751\n",
      "[19,   500] loss: 0.61301717\n",
      "[19,   600] loss: 0.63452239\n",
      "[19,   700] loss: 0.59927925\n",
      "[19,   800] loss: 0.58618604\n",
      "[19,   900] loss: 0.62059771\n",
      "[19,  1000] loss: 0.57900943\n",
      "[19,  1100] loss: 0.60495130\n",
      "[19,  1200] loss: 0.59546134\n",
      "[19,  1300] loss: 0.56644642\n",
      "[19,  1400] loss: 0.59683637\n",
      "[19,  1500] loss: 0.60561955\n",
      "[19,  1600] loss: 0.63069631\n",
      "[19,  1700] loss: 0.60604538\n",
      "[19,  1800] loss: 0.59115955\n",
      "[19,  1900] loss: 0.60491368\n",
      "[19,  2000] loss: 0.58656737\n",
      "[19,  2100] loss: 0.58750448\n",
      "[19,  2200] loss: 0.59628396\n",
      "[19,  2300] loss: 0.62923631\n",
      "[19,  2400] loss: 0.59353098\n",
      "[19,  2500] loss: 0.59578096\n",
      "[19,  2600] loss: 0.58004356\n",
      "[19,  2700] loss: 0.59892357\n",
      "[19,  2800] loss: 0.60524810\n",
      "[19,  2900] loss: 0.60401741\n",
      "test accuracy 0.7568276634647575\n",
      "[20,   100] loss: 0.58482616\n",
      "[20,   200] loss: 0.61431006\n",
      "[20,   300] loss: 0.58372675\n",
      "[20,   400] loss: 0.59379123\n",
      "[20,   500] loss: 0.59346400\n",
      "[20,   600] loss: 0.55981404\n",
      "[20,   700] loss: 0.62926998\n",
      "[20,   800] loss: 0.61845568\n",
      "[20,   900] loss: 0.58413165\n",
      "[20,  1000] loss: 0.59929490\n",
      "[20,  1100] loss: 0.57556046\n",
      "[20,  1200] loss: 0.60689511\n",
      "[20,  1300] loss: 0.59842810\n",
      "[20,  1400] loss: 0.61079195\n",
      "[20,  1500] loss: 0.60560675\n",
      "[20,  1600] loss: 0.62793578\n",
      "[20,  1700] loss: 0.58538825\n",
      "[20,  1800] loss: 0.57476650\n",
      "[20,  1900] loss: 0.58568975\n",
      "[20,  2000] loss: 0.61224940\n",
      "[20,  2100] loss: 0.57639714\n",
      "[20,  2200] loss: 0.58632467\n",
      "[20,  2300] loss: 0.60191417\n",
      "[20,  2400] loss: 0.57190858\n",
      "[20,  2500] loss: 0.59353393\n",
      "[20,  2600] loss: 0.60830969\n",
      "[20,  2700] loss: 0.57775494\n",
      "[20,  2800] loss: 0.61283228\n",
      "[20,  2900] loss: 0.60009060\n",
      "test accuracy 0.7564614933944528\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        model.train()\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        if model.__class__.__name__ == 'ViT':\n",
    "            inputs.unsqueeze_(axis=1)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.8f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    accs = []\n",
    "    for data in test_dataloader:\n",
    "        inputs, labels = data\n",
    "        if model.__class__.__name__ == 'ViT':\n",
    "            inputs.unsqueeze_(axis=1)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            predicted_labels = outputs.max(dim=1).indices\n",
    "\n",
    "        acc = accuracy_score(labels, predicted_labels)\n",
    "        accs.append(acc)\n",
    "\n",
    "    print('test accuracy', np.mean(accs))\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
